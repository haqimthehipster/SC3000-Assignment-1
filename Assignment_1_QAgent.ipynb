{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yuezhonggg/Balance-Pole-on-a-Cart-Reinforcement-Learning/blob/master/Assignment_1_(DQN)_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZauhjPSfX7pI"
      },
      "source": [
        "# Tutorial and Sample Code for Balancing a Pole on a Cart"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBiYOoesYMvr"
      },
      "source": [
        "## Installing dependencies:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PbgnVwZmX5uW",
        "outputId": "6bff3a98-c2e9-4f4f-8730-0d3eb89a63a2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The system cannot find the path specified.\n",
            "The system cannot find the path specified.\n",
            "The system cannot find the path specified.\n",
            "The system cannot find the path specified.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym[classic_control] in c:\\users\\yue zhong\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\yue zhong\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from gym[classic_control]) (1.24.2)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in c:\\users\\yue zhong\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from gym[classic_control]) (6.0.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in c:\\users\\yue zhong\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from gym[classic_control]) (0.0.8)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\yue zhong\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from gym[classic_control]) (2.2.1)\n",
            "Requirement already satisfied: pygame==2.1.0 in c:\\users\\yue zhong\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from gym[classic_control]) (2.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in c:\\users\\yue zhong\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from importlib-metadata>=4.8.0->gym[classic_control]) (3.15.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The system cannot find the path specified.\n",
            "The system cannot find the path specified.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: setuptools in c:\\users\\yue zhong\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (67.6.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The system cannot find the path specified.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow in c:\\users\\yue zhong\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (2.11.0)\n",
            "Requirement already satisfied: tensorflow-intel==2.11.0 in c:\\users\\yue zhong\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (2.11.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\yue zhong\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\yue zhong\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (15.0.6.1)\n",
            "Requirement already satisfied: packaging in c:\\users\\yue zhong\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\yue zhong\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (4.5.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\yue zhong\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.15.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\yue zhong\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (0.4.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\yue zhong\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\yue zhong\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.20 in c:\\users\\yue zhong\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.24.2)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\yue zhong\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (0.31.0)\n",
            "Requirement already satisfied: setuptools in c:\\users\\yue zhong\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (67.6.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\yue zhong\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.51.3)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\yue zhong\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.4.0)\n",
            "Requirement already satisfied: six>=1.12.0 in c:\\users\\yue zhong\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.16.0)\n",
            "Requirement already satisfied: keras<2.12,>=2.11.0 in c:\\users\\yue zhong\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.11.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\yue zhong\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\yue zhong\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (23.3.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in c:\\users\\yue zhong\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.11.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\yue zhong\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.19.6)\n",
            "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\yue zhong\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.8.0)\n",
            "Requirement already satisfied: tensorboard<2.12,>=2.11 in c:\\users\\yue zhong\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.11.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\yue zhong\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.11.0->tensorflow) (0.38.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\yue zhong\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\yue zhong\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.28.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\yue zhong\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\yue zhong\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\yue zhong\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\yue zhong\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.16.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\yue zhong\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\yue zhong\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\yue zhong\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\yue zhong\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (5.3.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\yue zhong\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\yue zhong\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (6.0.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\yue zhong\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\yue zhong\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\yue zhong\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.1.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\yue zhong\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.26.15)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\yue zhong\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in c:\\users\\yue zhong\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.15.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\yue zhong\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\yue zhong\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.2.2)\n",
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.2.2-cp39-cp39-win_amd64.whl (8.4 MB)\n",
            "     ---------------------------------------- 0.0/8.4 MB ? eta -:--:--\n",
            "     --- ------------------------------------ 0.8/8.4 MB 17.0 MB/s eta 0:00:01\n",
            "     ------------ --------------------------- 2.5/8.4 MB 26.7 MB/s eta 0:00:01\n",
            "     --------------------- ------------------ 4.6/8.4 MB 32.3 MB/s eta 0:00:01\n",
            "     ------------------------------- -------- 6.7/8.4 MB 38.6 MB/s eta 0:00:01\n",
            "     ---------------------------------------  8.4/8.4 MB 38.2 MB/s eta 0:00:01\n",
            "     ---------------------------------------- 8.4/8.4 MB 35.7 MB/s eta 0:00:00\n",
            "Collecting scipy>=1.3.2\n",
            "  Downloading scipy-1.10.1-cp39-cp39-win_amd64.whl (42.5 MB)\n",
            "     ---------------------------------------- 0.0/42.5 MB ? eta -:--:--\n",
            "     - -------------------------------------- 1.7/42.5 MB 55.2 MB/s eta 0:00:01\n",
            "     --- ------------------------------------ 3.8/42.5 MB 48.8 MB/s eta 0:00:01\n",
            "     ----- ---------------------------------- 5.9/42.5 MB 47.1 MB/s eta 0:00:01\n",
            "     ------- -------------------------------- 8.0/42.5 MB 46.6 MB/s eta 0:00:01\n",
            "     --------- ----------------------------- 10.0/42.5 MB 45.7 MB/s eta 0:00:01\n",
            "     ----------- --------------------------- 12.1/42.5 MB 43.7 MB/s eta 0:00:01\n",
            "     ------------- ------------------------- 14.2/42.5 MB 43.7 MB/s eta 0:00:01\n",
            "     -------------- ------------------------ 16.3/42.5 MB 43.7 MB/s eta 0:00:01\n",
            "     ---------------- ---------------------- 18.4/42.5 MB 46.9 MB/s eta 0:00:01\n",
            "     ------------------ -------------------- 20.4/42.5 MB 46.9 MB/s eta 0:00:01\n",
            "     -------------------- ------------------ 22.6/42.5 MB 46.7 MB/s eta 0:00:01\n",
            "     ---------------------- ---------------- 24.6/42.5 MB 46.7 MB/s eta 0:00:01\n",
            "     ------------------------ -------------- 26.7/42.5 MB 46.7 MB/s eta 0:00:01\n",
            "     -------------------------- ------------ 28.8/42.5 MB 46.7 MB/s eta 0:00:01\n",
            "     ---------------------------- ---------- 30.8/42.5 MB 46.7 MB/s eta 0:00:01\n",
            "     ------------------------------ -------- 32.9/42.5 MB 43.5 MB/s eta 0:00:01\n",
            "     -------------------------------- ------ 35.0/42.5 MB 43.7 MB/s eta 0:00:01\n",
            "     ---------------------------------- ---- 37.1/42.5 MB 43.7 MB/s eta 0:00:01\n",
            "     ----------------------------------- --- 39.1/42.5 MB 46.7 MB/s eta 0:00:01\n",
            "     ------------------------------------ -- 39.5/42.5 MB 46.7 MB/s eta 0:00:01\n",
            "     --------------------------------------  42.5/42.5 MB 43.7 MB/s eta 0:00:01\n",
            "     --------------------------------------  42.5/42.5 MB 43.7 MB/s eta 0:00:01\n",
            "     --------------------------------------- 42.5/42.5 MB 32.8 MB/s eta 0:00:00\n",
            "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\yue zhong\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-learn) (1.24.2)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
            "Collecting joblib>=1.1.1\n",
            "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
            "     ---------------------------------------- 0.0/298.0 kB ? eta -:--:--\n",
            "     ---------------------------------------- 298.0/298.0 kB ? eta 0:00:00\n",
            "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
            "Successfully installed joblib-1.2.0 scikit-learn-1.2.2 scipy-1.10.1 threadpoolctl-3.1.0\n"
          ]
        }
      ],
      "source": [
        "!apt-get install -y xvfb python-opengl > /dev/null 2>&1\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "!pip install gym[classic_control]\n",
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install cmake > /dev/null 2>&1\n",
        "!pip install --upgrade setuptools 2>&1\n",
        "!pip install ez_setup > /dev/null 2>&1\n",
        "!pip install tensorflow\n",
        "!pip install -U scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwKbYeTgbaTA"
      },
      "source": [
        "## Importing dependencies and define helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "j6KpgCLGYWmj"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import RecordVideo\n",
        "gymlogger.set_level(40) #error only\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "import tensorflow as tf\n",
        "from IPython.display import HTML\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehbqP9CXbmo7"
      },
      "source": [
        "## Tutorial: Loading CartPole environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Go12dH4qbwBy"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"CartPole-v1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XZ9g3xrcAXE"
      },
      "source": [
        "We can check the action and observation space of this environment. Discrete(2) means that there are two valid discrete actions: 0 & 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ytxvVmLdcRyw",
        "outputId": "ab631070-7c06-420b-ef20-a095d8eb444f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Discrete(2)\n"
          ]
        }
      ],
      "source": [
        "print(env.action_space)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVXGWi_Ncfg-"
      },
      "source": [
        "The observation space is given below. The first two arrays define the min and max values of the 4 observed values, corresponding to cart position, velocity and pole angle, angular velocity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DyqHr9I5cdkX",
        "outputId": "6cad64eb-b7fb-4fba-f7a2-d276dfdd6a4f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n"
          ]
        }
      ],
      "source": [
        "print(env.observation_space)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFOdaU2Gdyg0"
      },
      "source": [
        "We call each round of the pole-balancing game an \"episode\". At the start of each episode, make sure the environment is reset, which chooses a random initial state, e.g., pole slightly tilted to the right. This initialization can be achieved by the code below, which returns the observation of the initial state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMr6qAqxdOsm",
        "outputId": "4a9d873f-0e9c-4e42-9dc7-2f47b49c00a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial observations: (array([-0.04447088, -0.03478999, -0.0028578 ,  0.0465674 ], dtype=float32), {})\n"
          ]
        }
      ],
      "source": [
        "observation = env.reset()\n",
        "print(\"Initial observations:\", observation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnG2QdfbeZrI"
      },
      "source": [
        "For the CartPole environment, there are two possible actions: 0 for pushing to the left and 1 for pushing to the right. For example, we can push the cart to the left using code below, which returns the new observation, the current reward, an indicator of whether the game ends, and some additional information (not used in this project). For CartPole, the game ends when the pole is significantly tilted or you manage to balance the pole for 500 steps. You get exactly 1 reward for each step before the game ends (i.e., max cumulative reward is 500)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MmfMDvyYdWGk",
        "outputId": "cb6ef2a6-ef6b-4981-9fe2-987490a4e9e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "New observations after choosing action 0: [-0.04614231 -0.94072413  0.07200916  1.54641   ]\n",
            "Reward for this step: 1.0\n",
            "Is this round done? False\n",
            "{}\n"
          ]
        }
      ],
      "source": [
        "observation, reward, done, info , hi = env.step(0)\n",
        "print(\"New observations after choosing action 0:\", observation)\n",
        "print(\"Reward for this step:\", reward)\n",
        "print(\"Is this round done?\", done)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tj0zCh59fhBb"
      },
      "source": [
        "Now we can play a full round of the game using a naive strategy (always choosing action 0), and show the cumulative reward in the round. Note that reward returned by env.step(*) corresponds to the reward for current step. So we have to accumulate the reward for each step. Clearly, the naive strategy performs poorly by surviving only a dozen of steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVucQVRwf6Jm",
        "outputId": "427fe789-f4d6-4aac-9596-ffd8d0f4c24a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cumulative reward for this round: 10.0\n"
          ]
        }
      ],
      "source": [
        "observation = env.reset()\n",
        "cumulative_reward = 0\n",
        "done = False\n",
        "while not done:\n",
        "    observation, reward, done, info , redundant = env.step(0)\n",
        "    cumulative_reward += reward\n",
        "print(\"Cumulative reward for this round:\", cumulative_reward)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oIzK9SzhlWN"
      },
      "source": [
        "## Task 1: Development of an RL agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cc6_e5c_huiq"
      },
      "source": [
        "\n",
        "***Selection of GPU system to use***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "Hk-M4QEfh6l5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting sklearn\n",
            "  Downloading sklearn-0.0.post1.tar.gz (3.6 kB)\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Building wheels for collected packages: sklearn\n",
            "  Building wheel for sklearn (setup.py): started\n",
            "  Building wheel for sklearn (setup.py): finished with status 'done'\n",
            "  Created wheel for sklearn: filename=sklearn-0.0.post1-py3-none-any.whl size=2954 sha256=7f830005b01ef9306312cc409c823a52122212d8b57d83570e5f16f501b14dd9\n",
            "  Stored in directory: c:\\users\\yue zhong\\appdata\\local\\pip\\cache\\wheels\\f8\\e0\\3d\\9d0c2020c44a519b9f02ab4fa6d2a4a996c98d79ab2f569fa1\n",
            "Successfully built sklearn\n",
            "Installing collected packages: sklearn\n",
            "Successfully installed sklearn-0.0.post1\n"
          ]
        }
      ],
      "source": [
        "import os \n",
        "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHCdwqC4KZDg"
      },
      "source": [
        "***Importing Dependecies***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "id": "40avm9wBKY03"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import KBinsDiscretizer\n",
        "import numpy as np \n",
        "import time, math, random\n",
        "from typing import Tuple\n",
        "\n",
        "# import gym \n",
        "import gym"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3jCOcGi9cVzP"
      },
      "source": [
        "***Convert Catpoles continues state space into discrete one***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "GupS8QPacV7l"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"CartPole-v1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "id": "B_9OGXv0crjj"
      },
      "outputs": [],
      "source": [
        "n_bins = ( 6 , 12 )\n",
        "lower_bounds = [ env.observation_space.low[2], -math.radians(50) ]\n",
        "upper_bounds = [ env.observation_space.high[2], math.radians(50) ]\n",
        "\n",
        "def discretizer( _ , __ , angle, pole_velocity ) -> Tuple[int,...]:\n",
        "    \"\"\"Convert continues state intro a discrete state\"\"\"\n",
        "    est = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy='uniform')\n",
        "    est.fit([lower_bounds, upper_bounds ])\n",
        "    return tuple(map(int,est.transform([[angle, pole_velocity]])[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDHYcnciLTRs"
      },
      "source": [
        "***DQN Agent***"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Initialise the Q value table with zeros\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(6, 12, 2)"
            ]
          },
          "execution_count": 129,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Q_table = np.zeros(n_bins + (env.action_space.n,))\n",
        "Q_table.shape"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creating a policy for Q table to select the highest Q Value\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {},
      "outputs": [],
      "source": [
        "def policy( state : tuple ):\n",
        "    \"\"\"Choosing action based on epsilon-greedy policy\"\"\"\n",
        "    return np.argmax(Q_table[state])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*Update* Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {},
      "outputs": [],
      "source": [
        "def new_Q_value( reward : float ,  new_state : tuple , discount_factor=1 ) -> float:\n",
        "    \"\"\"Temperal diffrence for updating Q-value of state-action pair\"\"\"\n",
        "    future_optimal_value = np.max(Q_table[new_state])\n",
        "    learned_value = reward + discount_factor * future_optimal_value\n",
        "    return learned_value"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Decaying Learning rate and Exploration rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Adaptive learning of Learning Rate\n",
        "def learning_rate(n : int , min_rate=0.01 ) -> float  :\n",
        "    \"\"\"Decaying learning rate\"\"\"\n",
        "    return max(min_rate, min(1.0, 1.0 - math.log10((n + 1) / 25)))\n",
        "\n",
        "def exploration_rate(n : int, min_rate= 0.1 ) -> float :\n",
        "    \"\"\"Decaying exploration rate\"\"\"\n",
        "    return max(min_rate, min(1, 1.0 - math.log10((n  + 1) / 25)))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "id": "pYmvu2JVA7rR"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "discretizer() missing 2 required positional arguments: 'angle' and 'pole_velocity'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[138], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m n_episodes \u001b[39m=\u001b[39m \u001b[39m10000\u001b[39m \n\u001b[0;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_episodes):\n\u001b[0;32m      3\u001b[0m     \n\u001b[0;32m      4\u001b[0m     \u001b[39m# Siscretize state into buckets\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m     current_state, done \u001b[39m=\u001b[39m discretizer(\u001b[39m*\u001b[39;49menv\u001b[39m.\u001b[39;49mreset()), \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     \u001b[39mwhile\u001b[39;00m done\u001b[39m==\u001b[39m\u001b[39mFalse\u001b[39;00m:\n\u001b[0;32m      8\u001b[0m         \n\u001b[0;32m      9\u001b[0m         \u001b[39m# policy action \u001b[39;00m\n\u001b[0;32m     10\u001b[0m         action \u001b[39m=\u001b[39m policy(current_state) \u001b[39m# exploit\u001b[39;00m\n",
            "\u001b[1;31mTypeError\u001b[0m: discretizer() missing 2 required positional arguments: 'angle' and 'pole_velocity'"
          ]
        }
      ],
      "source": [
        "n_episodes = 10000 \n",
        "for e in range(n_episodes):\n",
        "    \n",
        "    # Siscretize state into buckets\n",
        "    current_state, done = discretizer(*env.reset()), False\n",
        "    \n",
        "    while done==False:\n",
        "        \n",
        "        # policy action \n",
        "        action = policy(current_state) # exploit\n",
        "        \n",
        "        # insert random action\n",
        "        if np.random.random() < exploration_rate(e) : \n",
        "            action = env.action_space.sample() # explore \n",
        "         \n",
        "        # increment enviroment\n",
        "        obs, reward, done, _ = env.step(action)\n",
        "        new_state = discretizer(*obs)\n",
        "        \n",
        "        # Update Q-Table\n",
        "        lr = learning_rate(e)\n",
        "        learnt_value = new_Q_value(reward , new_state )\n",
        "        old_value = Q_table[current_state][action]\n",
        "        Q_table[current_state][action] = (1-lr)*old_value + lr*learnt_value\n",
        "        \n",
        "        current_state = new_state\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDLOCVlTSSut"
      },
      "source": [
        "***Training of DQN Agent*** Took 52 mins for 300 episodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-WIZA-rJOeKC",
        "outputId": "e32cbfcb-9cab-4eac-bc7f-b458a182f69f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "episode: 0/1000, score: 12, e: 1.0\n",
            "episode: 1/1000, score: 12, e: 1.0\n",
            "episode: 2/1000, score: 28, e: 1.0\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 12ms/step\n",
            "1/1 [==============================] - 0s 11ms/step\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 4) for input KerasTensor(type_spec=TensorSpec(shape=(None, 4), dtype=tf.float32, name='dense_141_input'), name='dense_141_input', description=\"created by layer 'dense_141_input'\"), but it was called on an input with incompatible shape (None,).\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Model was constructed with shape (None, 4) for input KerasTensor(type_spec=TensorSpec(shape=(None, 4), dtype=tf.float32, name='dense_141_input'), name='dense_141_input', description=\"created by layer 'dense_141_input'\"), but it was called on an input with incompatible shape (None,).\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "in user code:\n\n    File \"c:\\Users\\Yue Zhong\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py\", line 2137, in predict_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\Yue Zhong\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py\", line 2123, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\Yue Zhong\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py\", line 2111, in run_step  **\n        outputs = model.predict_step(data)\n    File \"c:\\Users\\Yue Zhong\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py\", line 2079, in predict_step\n        return self(x, training=False)\n    File \"c:\\Users\\Yue Zhong\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\Yue Zhong\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 250, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer 'sequential_47' (type Sequential).\n    \n    Input 0 of layer \"dense_141\" is incompatible with the layer: expected min_ndim=2, found ndim=1. Full shape received: (None,)\n    \n    Call arguments received by layer 'sequential_47' (type Sequential):\n      • inputs=('tf.Tensor(shape=(None,), dtype=float32)', {})\n      • training=False\n      • mask=None\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[151], line 74\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(agent\u001b[39m.\u001b[39mmemory) \u001b[39m>\u001b[39m batch_size:\n\u001b[1;32m---> 74\u001b[0m     agent\u001b[39m.\u001b[39;49mreplay(batch_size)\n",
            "Cell \u001b[1;32mIn[151], line 46\u001b[0m, in \u001b[0;36mDQNAgent.replay\u001b[1;34m(self, batch_size)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m done:\n\u001b[0;32m     44\u001b[0m     target \u001b[39m=\u001b[39m (reward \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgamma \u001b[39m*\u001b[39m\n\u001b[0;32m     45\u001b[0m               np\u001b[39m.\u001b[39mamax(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mpredict(next_state)[\u001b[39m0\u001b[39m]))\n\u001b[1;32m---> 46\u001b[0m target_f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mpredict(state)\n\u001b[0;32m     47\u001b[0m target_f[\u001b[39m0\u001b[39m][action] \u001b[39m=\u001b[39m target\n\u001b[0;32m     48\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mfit(state, target_f, epochs\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\Yue Zhong\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
            "File \u001b[1;32mC:\\Users\\YUEZHO~1\\AppData\\Local\\Temp\\__autograph_generated_filen3_ufkjy.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__predict_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
            "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\Yue Zhong\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py\", line 2137, in predict_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\Yue Zhong\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py\", line 2123, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\Yue Zhong\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py\", line 2111, in run_step  **\n        outputs = model.predict_step(data)\n    File \"c:\\Users\\Yue Zhong\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py\", line 2079, in predict_step\n        return self(x, training=False)\n    File \"c:\\Users\\Yue Zhong\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\Yue Zhong\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 250, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer 'sequential_47' (type Sequential).\n    \n    Input 0 of layer \"dense_141\" is incompatible with the layer: expected min_ndim=2, found ndim=1. Full shape received: (None,)\n    \n    Call arguments received by layer 'sequential_47' (type Sequential):\n      • inputs=('tf.Tensor(shape=(None,), dtype=float32)', {})\n      • training=False\n      • mask=None\n"
          ]
        }
      ],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import deque\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_size, action_size):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=2000)\n",
        "        self.gamma = 0.95  # discount rate\n",
        "        self.epsilon = 1.0  # exploration rate\n",
        "        self.epsilon_min = 0.01\n",
        "        self.epsilon_decay = 0.995\n",
        "        self.learning_rate = 0.001\n",
        "        self.model = self._build_model()\n",
        "\n",
        "    def _build_model(self):\n",
        "        # Neural Net for Deep-Q learning Model\n",
        "        model = Sequential()\n",
        "        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
        "        model.add(Dense(24, activation='relu'))\n",
        "        model.add(Dense(self.action_size, activation='linear'))\n",
        "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
        "        return model\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def act(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return env.action_space.sample()\n",
        "        act_values = self.model.predict(state)\n",
        "        return np.argmax(act_values[0])  # returns action\n",
        "\n",
        "    def replay(self, batch_size):\n",
        "        minibatch = random.sample(self.memory, batch_size)\n",
        "        for state, action, reward, next_state, done in minibatch:\n",
        "            target = reward\n",
        "            if not done:\n",
        "                target = (reward + self.gamma *\n",
        "                          np.amax(self.model.predict(next_state)[0]))\n",
        "            target_f = self.model.predict(state)\n",
        "            target_f[0][action] = target\n",
        "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    env = gym.make('CartPole-v1')\n",
        "    state_size = env.observation_space.shape[0]\n",
        "    action_size = env.action_space.n\n",
        "    agent = DQNAgent(state_size, action_size)\n",
        "    batch_size = 32\n",
        "    num_episodes = 1000\n",
        "\n",
        "    for e in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        for time in range(500):\n",
        "            action = agent.act(state)\n",
        "            next_state, reward, done, _  , _= env.step(action)\n",
        "            reward = reward if not done else -10\n",
        "            next_state = np.reshape(next_state, [1, state_size])\n",
        "            agent.remember(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "            if done:\n",
        "                print(\"episode: {}/{}, score: {}, e: {:.2}\"\n",
        "                      .format(e, num_episodes, time, agent.epsilon))\n",
        "                break\n",
        "        if len(agent.memory) > batch_size:\n",
        "            agent.replay(batch_size)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAi7KKwNiegR"
      },
      "source": [
        "For Task 1, we can show the observation and chosen action below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "ae2ia-vUiNKJ"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[31], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m observation \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mreset()\n\u001b[1;32m----> 2\u001b[0m observation \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mreshape(observation, [\u001b[39m1\u001b[39;49m, state_size])\n\u001b[0;32m      3\u001b[0m action \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39mact(observation)\n\u001b[0;32m      4\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mObservation:\u001b[39m\u001b[39m\"\u001b[39m, observation)\n",
            "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mreshape\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
            "File \u001b[1;32mc:\\Users\\Yue Zhong\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\core\\fromnumeric.py:298\u001b[0m, in \u001b[0;36mreshape\u001b[1;34m(a, newshape, order)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[39m@array_function_dispatch\u001b[39m(_reshape_dispatcher)\n\u001b[0;32m    199\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreshape\u001b[39m(a, newshape, order\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mC\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m    200\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \u001b[39m    Gives a new shape to an array without changing its data.\u001b[39;00m\n\u001b[0;32m    202\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    296\u001b[0m \u001b[39m           [5, 6]])\u001b[39;00m\n\u001b[0;32m    297\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 298\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapfunc(a, \u001b[39m'\u001b[39;49m\u001b[39mreshape\u001b[39;49m\u001b[39m'\u001b[39;49m, newshape, order\u001b[39m=\u001b[39;49morder)\n",
            "File \u001b[1;32mc:\\Users\\Yue Zhong\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\core\\fromnumeric.py:54\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     52\u001b[0m bound \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(obj, method, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m     53\u001b[0m \u001b[39mif\u001b[39;00m bound \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> 54\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapit(obj, method, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m     56\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     57\u001b[0m     \u001b[39mreturn\u001b[39;00m bound(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n",
            "File \u001b[1;32mc:\\Users\\Yue Zhong\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\core\\fromnumeric.py:43\u001b[0m, in \u001b[0;36m_wrapit\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m:\n\u001b[0;32m     42\u001b[0m     wrap \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(asarray(obj), method)(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m     44\u001b[0m \u001b[39mif\u001b[39;00m wrap:\n\u001b[0;32m     45\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(result, mu\u001b[39m.\u001b[39mndarray):\n",
            "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part."
          ]
        }
      ],
      "source": [
        "observation = env.reset()\n",
        "observation = np.reshape(observation, [1, state_size])\n",
        "action = agent.act(observation)\n",
        "print(\"Observation:\", observation)\n",
        "print(\"Chosen action:\", action)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XtIQ0Rti1gm"
      },
      "source": [
        "## Task 2: Demonstrate the effectiveness of the RL agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djBEShf0kGI4"
      },
      "source": [
        "For this task, use the agent developed in Task 1 to play the game for 100 episodes (refer to tutorial for how to play a round), record the cumulative reward for each round, and plot the reward for each round. A sample plotting code is given below. Note that you must include code to play for 100 episodes and use the code to obtain round_results for plotting. DO NOT record the round results in advance and paste the results to the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RZrCKywQi6CE"
      },
      "outputs": [],
      "source": [
        "t2 = 100\n",
        "episode_results = []\n",
        "for episode in range(t2):\n",
        "  done = False\n",
        "  total = 0\n",
        "  observation = env.reset()\n",
        "  observation = np.reshape(observation, [1, state_size])\n",
        "  while not done:\n",
        "    action = agent.act(observation)\n",
        "    next_state, reward, done, info = env.step(action)\n",
        "    total += reward\n",
        "    if done:\n",
        "      break\n",
        "    \n",
        "  episode_results.append(total)\n",
        "\n",
        "plt.plot(episode_results)\n",
        "plt.title('Cumulative reward for each episode')\n",
        "plt.ylabel('Cumulative reward')\n",
        "plt.xlabel('episode')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XndSYH7wlvn7"
      },
      "source": [
        "Print the average reward over the 100 episodes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pOiOp9OYlo5Y"
      },
      "outputs": [],
      "source": [
        "print(\"Average cumulative reward:\", sum(episode_results)/len(episode_results))\n",
        "print(\"Is my agent good enough?\", sum(episode_results)/len(episode_results) > 195)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vVpqcUPf8dGW"
      },
      "outputs": [],
      "source": [
        "def rand_policy_agent(observation):\n",
        "    return random.randint(0, 1)\n",
        "     "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yg0DCT38lFA6"
      },
      "source": [
        "## Task 3: Render one episode played by the agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vx1awMr9lc_w"
      },
      "source": [
        "Plug your agent to the code below to obtain rendered result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LYyavfbIa47D"
      },
      "outputs": [],
      "source": [
        "env = RecordVideo(gym.make(\"CartPole-v1\"), \"./video\")\n",
        "observation = env.reset()\n",
        "state = discretize_state(observation)\n",
        "print(observation)\n",
        "print(state)\n",
        "while True:\n",
        "    env.render()\n",
        "    #agent goes here\n",
        "    action = np.argmax(q_table[state])\n",
        "    print(action)\n",
        "    observation, reward, done, info = env.step(action) \n",
        "    if done: \n",
        "      break;    \n",
        "env.close()\n",
        "show_video()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kyY0ZCCb0wUy"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
